import re

class RussianTokenizer:
    def __init__(self):
        self.stop_words = set([
            "и", "в", "во", "не", "что", "он", "на", "я", "с", "со",
            "как", "а", "то", "все", "она", "так", "его", "но", "да",
            "ты", "к", "у", "же", "вы", "за", "бы", "по", "только",
            "ее", "мне", "было", "вот", "от", "меня", "еще", "нет",
            "о", "из", "ему", "теперь", "когда", "даже", "ну", "вдруг",
            "ли", "если", "уже", "или", "ни", "быть", "был", "него",
            "до", "вас", "нибудь", "опять", "уж", "вам", "ведь", "там",
            "потом", "себя", "ничего", "ей", "может", "они", "тут",
            "где", "есть", "надо", "ней", "для", "мы", "тебя", "их",
            "чем", "была", "сам", "чтоб", "без", "будто", "чего",
            "раз", "тоже", "себе", "под", "будет", "ж", "тогда",
            "кто", "этот", "того", "потому", "этого", "какой", "совсем",
            "ним", "здесь", "этом", "один", "почти", "мой", "тем",
            "чтобы", "нее", "кажется", "ниже", "каждый"
        ])

    def tokenize(self, text):
        tokens = re.findall(r'\b[а-яё]+\b', text.lower())
        filtered_tokens = [t for t in tokens if t not in self.stop_words]
        return filtered_tokens


# Пример использования
if __name__ == "__main__":
    tokenizer = RussianTokenizer()
    text = "Почему небо такое голубое и чистое?"
    tokens = tokenizer.tokenize(text)
    print(tokens)  # Выведет: ['почему', 'небо', 'такое', 'голубое', 'чистое']
